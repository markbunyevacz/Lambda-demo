---
description: 
globs: 
alwaysApply: false
---

## 1. PROJEKT √ÅTTEKINT√âS

### 1.1 Minimum Credible Product (MCP) C√©lja
**üéØ C√©l:** Bemutatni a rendszer alapvet≈ë k√©pess√©g√©t: egy felhaszn√°l√≥ term√©szetes nyelven kereshet √©p√≠t≈ëanyagot, a rendszer pedig a naprak√©sz, "scraped" adatok alapj√°n relev√°ns term√©keket √©s AI-alap√∫ szak√©rt≈ëi v√°laszt ad.

### 1.2 MCP Scope
**‚úÖ R√©sze az MCP-nek:**
- **Gy√°rt√≥k**: Csak 1 gy√°rt√≥ (Rockwool) adatainak teljes feldolgoz√°sa
- **Hibrid adatgy≈±jt√©s**: Hagyom√°nyos scraper + **üöÄ BrightData MCP AI** (48 tools, CAPTCHA solving)
- **Adatb√°zisok**: Struktur√°lt adatb√°zis (PostgreSQL) √©s vektor adatb√°zis (Chroma)
- **Keres√©s**: Term√©szetes nyelv≈± keres√©s (RAG pipeline) √©s egyszer≈± term√©klista
- **UI**: Egy keres≈ëmez≈ë, egy AI chat ablak √©s egy term√©klista megjelen√≠t≈ë

**‚ùå Nem r√©sze az MCP-nek:**
- T√∂bb gy√°rt√≥ t√°mogat√°sa
- Aj√°nlatk√©sz√≠t≈ë modul
- Komplex sz≈±r≈ëk
- Felhaszn√°l√≥i fi√≥kok

---

## 2. AKTU√ÅLIS ST√ÅTUSZ

### 2.1 ‚úÖ PRODUCTION COMPLETE (Verified & Tested)
**Total: 3 major modules completed to production standard**

#### **Rockwool Term√©kadatlap Scraper** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: 45 product datasheets downloaded with 100% success rate
- **Tested**: `rockwool_scraper_final.py` - End-to-end production testing completed
- **Location**: `downloads/final_test/` (34 unique) + `downloads/final_test/duplicates/` (11 duplicates)
- **Features**: Fresh debug file refresh, smart duplicate handling, zero data loss
- **Performance**: 45 products found, 45 downloads successful, 0 failed

#### **Rockwool √Årlist√°k Scraper** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: 12 brochures and pricelists downloaded with 100% success rate
- **Tested**: `backend/app/scrapers/rockwool_final/brochure_scraper.py`
- **Location**: `downloads/rockwool_brochures/` (12 unique files, 0 duplicates)
- **Key Documents**: ROCKWOOL √Årlista 2025 (8.48 MB), ProRox √Årlista (6.61 MB)
- **Enhancement**: Applied proven success methodology

#### **Database Integration System** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: 46 ROCKWOOL products successfully integrated into PostgreSQL
- **Tested**: `demo_database_integration.py` - Full end-to-end production testing completed (2025-06-30)
- **Database**: PostgreSQL running in Docker with complete product catalog
- **Features**: Real manufacturer (ROCKWOOL), 6 product categories, smart categorization
- **API**: Live database accessible via FastAPI at `http://localhost:8000/products`
- **Performance**: 46/46 products created successfully, 0 failures, real-time verification completed

### 2.2 ‚úÖ INFRASTRUCTURE READY (Awaiting Verification)
**Total: 42 components with infrastructure completed**

#### **Alapinfrastrukt√∫ra** ‚úÖ
- Projektstrukt√∫ra fel√°ll√≠t√°sa (Backend, Docker)
- Docker-compose konfigur√°ci√≥ (FastAPI, PostgreSQL, Redis)
- Adatb√°zis s√©m√°k √©s SQLAlchemy modellek
- Alapvet≈ë FastAPI alkalmaz√°s (CORS √©s DB kapcsolattal)
- Alapvet≈ë Next.js/React frontend v√°z

#### **BrightData MCP AI Scraping Rendszer** ‚úÖ INFRASTRUCTURE READY
- **Evidence**: Connection to MCP server successful, all 48 tools loaded
- **Tested**: Standalone scripts confirmed MCP server initialization
- **Requires**: Human verification for full AI-driven scraping task

#### **Adatb√°zis √©s Automatiz√°l√°s** ‚úÖ INFRASTRUCTURE READY
- **Adatb√°zis Integr√°ci√≥s Logika**: All data models and integration services defined
- **Celery √©s Celery Beat**: All tasks and scheduling configurations defined
- **Integr√°ci√≥s Tesztel√©s**: `test_integration.py` framework exists

### 2.3 üîÑ IN DEVELOPMENT (Current Priority)
**Immediate Next Steps:**
1. **Client-Specific Architecture** - Modular design implementation for multiple manufacturers
2. **Factory Pattern Implementation** - Reusable scraper framework development
3. **RAG Pipeline Foundation** - Chroma vector database initialization with existing data

### 2.4 üîç AWAITING HUMAN VERIFICATION
**Critical Items Requiring Testing:**
1. **BrightData MCP Integration** - Production testing required for full AI-driven scraping
2. **Celery Automation** - Production scheduling verification for automated tasks

---

## 3. FEJLESZT√âSI F√ÅZISOK

### 3.1 F√°zis 1: Alapoz√°s √©s Infrastrukt√∫ra ‚úÖ VERIFIED COMPLETE
- [x] Projektstrukt√∫ra fel√°ll√≠t√°sa (Backend, Docker)
- [x] Docker-compose konfigur√°ci√≥ (FastAPI, PostgreSQL, Redis)
- [x] Adatb√°zis s√©m√°k √©s SQLAlchemy modellek l√©trehoz√°sa
- [x] Alapvet≈ë FastAPI alkalmaz√°s l√©trehoz√°sa, CORS √©s DB kapcsolattal
- [x] Alapvet≈ë Next.js/React frontend v√°z l√©trehoz√°sa

### 3.2 F√°zis 2: Adat-pipeline √©s Web Scraping ‚úÖ PRODUCTION READY
- [x] **Rockwool Term√©kadatlap Scraper** ‚úÖ PRODUCTION COMPLETE
- [x] **Rockwool √Årlist√°k Scraper** ‚úÖ PRODUCTION COMPLETE
- [x] **BrightData MCP AI Scraping Rendszer** ‚úÖ INFRASTRUCTURE READY
- [x] **Adatb√°zis Integr√°ci√≥s Logika** ‚úÖ INFRASTRUCTURE READY
- [x] **Celery √©s Celery Beat Id≈ëz√≠t√©s** ‚úÖ INFRASTRUCTURE READY
- [x] **Integr√°ci√≥s Tesztel√©s** ‚úÖ INFRASTRUCTURE READY
- [ ] **Leier scraper implement√°ci√≥ja** üîÑ PLANNED
- [ ] **Baumit scraper implement√°ci√≥ja** üîÑ PLANNED

### 3.3 F√°zis 3: AI Modul - RAG Pipeline üîÑ PLANNED
#### **Hibrid Vektor Adatb√°zis**
- [ ] **Hibrid Chroma vektor adatb√°zis** inicializ√°l√°sa √©s perziszt√°l√°sa
- [ ] Hagyom√°nyos scraped adatok vektoriz√°l√°sa
- [ ] **üöÄ AI-enhanced term√©kle√≠r√°sok** indexel√©se BrightData MCP eredm√©nyekb≈ël
- [ ] Magyar nyelvi embeddings optimaliz√°l√°s

#### **LangChain Integr√°ci√≥**
- [ ] **LangChain integr√°ci√≥ tov√°bbfejlesztve** √©s `BuildingMaterialsAI` service l√©trehoz√°sa
- [ ] **AI scraping context** integr√°l√°sa a RAG pipeline-ba
- [ ] **Intelligent retrieval** - forr√°s preferenci√°k (API vs MCP AI)
- [ ] **Real-time scraping capability** term√©szetes nyelv≈± k√©r√©sekre

#### **AI K√©pess√©gek**
- [ ] Term√©kadatok automatikus vektoriz√°l√°sa √©s indexel√©se (hibrid forr√°sok)
- [ ] Q&A l√°nc l√©trehoz√°sa a magyar nyelv≈±, √©p√≠t√©szeti szak√©rt≈ëi prompt-tal
- [ ] **üöÄ AI confidence score** alap√∫ term√©k rangsorol√°s √©s v√°laszmin≈ës√©g
- [ ] Kompatibilit√°s-ellen≈ërz≈ë logika alapjainak implement√°l√°sa (`check_system_compatibility`)

### 3.4 F√°zis 4: Backend API √©s Frontend Integr√°ci√≥ üîÑ PLANNED
#### **Hibrid API V√©gpontok**
- [ ] **Hibrid term√©kkeres≈ë API** v√©gpont l√©trehoz√°sa (sz√∂veges keres√©s + sz≈±r√©s)
- [ ] **üöÄ `/api/search/hybrid`** - Kombin√°lja hagyom√°nyos + AI eredm√©nyeket
- [ ] **üöÄ `/api/scraping/intelligent`** - Real-time AI scraping k√©r√©sre
- [ ] **üöÄ `/api/scraping/status`** - AI scraping feladatok monitoring

#### **AI Chat API**
- [ ] **AI Chat API tov√°bbfejlesztve** (`get_product_recommendations`)
- [ ] **üöÄ `/api/ai/analyze-product`** - Term√©k AI elemz√©s k√©r√©sre
- [ ] BrightData MCP eredm√©nyek integr√°l√°sa v√°laszokba
- [ ] Kompatibilit√°s-ellen≈ërz≈ë API v√©gpont l√©trehoz√°sa (AI-enhanced)

#### **Frontend Fejleszt√©sek**
- [ ] **Frontend: AI-enhanced term√©kkeres≈ë** interface fejleszt√©se
- [ ] **üöÄ Real-time scraping status** indicator
- [ ] **üöÄ AI confidence score** megjelen√≠t√©s term√©kekn√©l
- [ ] **üöÄ "K√©rd el AI-t√≥l √∫j term√©keket"** funkci√≥
- [ ] Frontend: AI Chat komponens fejleszt√©se √©s bek√∂t√©se
- [ ] Frontend: Term√©k adatlap √©s √∂sszehasonl√≠t√≥ komponens fejleszt√©se
- [ ] **üöÄ Scraping admin panel** - strat√©gia v√°lt√°s, monitoring (fejleszt≈ëknek)

### 3.5 F√°zis 5: Aj√°nlatk√©sz√≠t≈ë Modul üîÑ PLANNED
- [ ] `quotes` adatb√°zis t√°bla √©s SQLAlchemy modell
- [ ] `QuoteCalculator` service implement√°l√°sa (√°rkalkul√°ci√≥, vesztes√©gsz√°m√≠t√°s)
- [ ] Aj√°nlatk√©sz√≠t≈ë API v√©gpontok (l√©trehoz√°s, lek√©rdez√©s)
- [ ] PDF gener√°l√≥ modul integr√°l√°sa (ReportLab)
- [ ] Email k√ºld≈ë szolg√°ltat√°s integr√°ci√≥ja
- [ ] Frontend: Aj√°nlatk√©sz√≠t≈ë UI (kos√°r, v√©gleges√≠t√©s)

### 3.6 F√°zis 6: Finaliz√°l√°s √©s Deployment üîÑ PLANNED
- [ ] Teljes k√∂r≈± API √©s frontend tesztel√©s (pytest, Jest/Playwright)
- [ ] `Dockerfile`-ok finomhangol√°sa √©les k√∂rnyezetre
- [ ] CI/CD pipeline alapjainak l√©trehoz√°sa (pl. GitHub Actions)
- [ ] R√©szletes dokument√°ci√≥ (API Swagger, README)
- [ ] Felhaszn√°l√≥i tesztel√©s √©s visszajelz√©sek feldolgoz√°sa

---

## 4. AI AGENT FEJLESZT√âSEK

### 4.1 ü§ñ AI Agent Implement√°ci√≥s Backlog

#### **Adatfeldolgoz√≥ Agent** (`DataProcessingAgent`)
- [ ] Raw adatok normaliz√°l√°sa √©s tiszt√≠t√°sa
- [ ] Duplik√°tumok intelligens elt√°vol√≠t√°sa
- [ ] Kategoriz√°l√°s √©s c√≠mk√©z√©s automatiz√°l√°sa
- [ ] Min≈ës√©gbiztos√≠t√°si pipeline implement√°l√°sa

#### **Aj√°nl√°si Agent - RAG** (`RecommendationAgent`)
- [ ] Term√©k √∂sszehasonl√≠t√°si logika
- [ ] Szem√©lyre szabott aj√°nl√°sok gener√°l√°sa
- [ ] RAG alap√∫ v√°laszgener√°l√°s implement√°l√°sa
- [ ] Kontextus meg√©rt√©s √©s mem√≥ria kezel√©s

#### **√Årfigyel≈ë Agent** (`PriceMonitoringAgent`)
- [ ] √År tracking k√ºl√∂nb√∂z≈ë forr√°sokon kereszt√ºl
- [ ] Trend anal√≠zis √©s el≈ërejelz√©s
- [ ] Riaszt√°sok gener√°l√°sa √°r v√°ltoz√°sokra
- [ ] Historikus adatok kezel√©se √©s archiv√°l√°sa

#### **Kompatibilit√°si Agent** (`CompatibilityAgent`)
- [ ] Term√©kek kompatibilit√°s√°nak automatikus ellen≈ërz√©se
- [ ] M≈±szaki specifik√°ci√≥k √∂sszehasonl√≠t√°sa
- [ ] Alkalmaz√°si ter√ºletek elemz√©se
- [ ] Szabv√°nyok √©s el≈ë√≠r√°sok ellen≈ërz√©se

### 4.2 üèóÔ∏è Agent Infrastrukt√∫ra Fejleszt√©sek

#### **Event Bus Integr√°ci√≥** (`AgentEventBus`)
- [ ] Agent-to-agent kommunik√°ci√≥s rendszer
- [ ] Event subscription/publishing mechanizmus
- [ ] Message queuing (Redis) integr√°ci√≥
- [ ] Error handling √©s retry logika

#### **Agent Health Monitoring** (`AgentHealthMonitor`)
- [ ] Heartbeat ellen≈ërz√©si rendszer
- [ ] Performance metrik√°k gy≈±jt√©se
- [ ] Resource haszn√°lat tracking
- [ ] Automatikus √∫jraind√≠t√°si mechanizmus

#### **Agent Lifecycle Management**
- [ ] State management (INITIALIZING ‚Üí IDLE ‚Üí WORKING ‚Üí ERROR/STOPPING ‚Üí STOPPED)
- [ ] Graceful shutdown √©s restart
- [ ] Configuration hot-reload
- [ ] Service discovery implement√°l√°sa

#### **Performance Metrik√°k Rendszer** (`AgentMetrics`)
- [ ] Task completion rate tracking
- [ ] Average response time m√©r√©se
- [ ] Error rate monitoring
- [ ] Resource consumption analysis
- [ ] System-wide metrik√°k (total active agents, message throughput)

### 4.3 üõ†Ô∏è Fejleszt≈ëi Tooling √©s Template-ek

#### **Agent Template Script** implement√°l√°sa
- [ ] `python scripts/create_agent.py --name MyNewAgent --type data_processing`
- [ ] Skeleton k√≥dgener√°l√°s
- [ ] Konfigur√°ci√≥ template-ek
- [ ] Testing boilerplate gener√°l√°sa

#### **Integration Checklist** automatiz√°l√°sa
- [ ] Agent oszt√°ly implement√°l√°s ellen≈ërz√©se
- [ ] Health check endpoint valid√°l√°sa
- [ ] Metrics collection tesztel√©se
- [ ] Unit √©s integration tesztek futtat√°sa

#### **Migration Protocol** implement√°l√°sa
- [ ] Client-specific architecture √°t√°ll√°si l√©p√©sek
- [ ] Modular scraper design pattern alkalmaz√°sa
- [ ] Factory pattern implement√°l√°sa (`ClientFactory.create_scraper()`)
- [ ] Clean API interface l√©trehoz√°sa

---

## 5. K√ñVETKEZ≈ê L√âP√âSEK

### 5.1 üéØ Azonnal Megval√≥s√≠tand√≥ (H√©t 1-2)
1. **RAG Pipeline Foundation** - Chroma vector database initialization with existing 46 products
2. **BrightData MCP Production Testing** - Full AI-driven scraping task verification
3. **Client-Specific Architecture** - Modular design implementation for multiple manufacturers

### 5.2 üîÑ R√∂vid T√°v√∫ (H√©t 3-4)
1. **Factory Pattern Implementation** - Reusable scraper framework
2. **Celery Automation Testing** - Production scheduling verification
3. **RAG Pipeline Foundation** - Chroma vector database initialization

### 5.3 üìã K√∂zepes T√°v√∫ (H√©t 5-8)
1. **AI Agent Infrastructure** - Event Bus, Health Monitoring
2. **Hibrid API Endpoints** - Intelligent search and scraping
3. **Frontend AI Integration** - Real-time status, confidence scores

---

## 6. PROVEN METODOL√ìGIA

### 6.1 üéØ SUCCESS PATTERN IDENTIFIED
1. **Evidence-First Approach** - Start with existing working data (debug files)
2. **Incremental Testing** - Test each component individually before integration
3. **Zero Data Loss** - Implement duplicate handling, never overwrite
4. **Fresh Data Strategy** - Auto-refresh content before scraping
5. **Production Validation** - Complete end-to-end testing before marking complete

### 6.2 üîë Key Insights
**Working Code ‚â† Infrastructure Existence**
- Only mark items as PRODUCTION COMPLETE after successful end-to-end testing
- Use "INFRASTRUCTURE READY" for components that exist but lack verification
- Prioritize functional validation over feature expansion

### 6.3 üìä St√°tusz Kateg√≥ri√°k
- **‚úÖ PRODUCTION COMPLETE**: Verified & Tested, ready for production use
- **‚úÖ INFRASTRUCTURE READY**: Code exists, basic tests pass, awaiting verification
- **üîÑ IN DEVELOPMENT**: Active development in progress
- **üîç AWAITING VERIFICATION**: Human testing required
- **üîÑ PLANNED**: Design/requirements ready, not yet started

---

## üìö GYORS REFERENCIA

### Legfontosabb St√°tuszok
- **Production Complete**: 3 modulok (Rockwool scrapers + Database Integration)
- **Infrastructure Ready**: 42 komponens
- **Awaiting Verification**: 2 kritikus elem
- **Planned**: 22+ komponens

### K√∂vetkez≈ë Milestone
**üéØ RAG Pipeline Foundation** - 46 ROCKWOOL term√©k vektoriz√°l√°sa √©s Chroma adatb√°zis inicializ√°l√°sa

VERIFICATION CHECKLIST:
‚ñ° Did I scan the entire document systematically?
‚ñ° Did I check all major sections and subsections?
‚ñ° Am I confusing similar but different items?
‚ñ° Have I verified my findings against the original request?
‚ñ° If I found nothing, did I complete a full document sweep?

CONSISTENCY PROTOCOL:
1. Initial reading and conclusion
2. Wait/pause briefly
3. Re-approach the same question independently
4. Compare both conclusions
5. If inconsistent, perform tie-breaker third reading

WHEN ERRORS OCCUR:
1. Acknowledge the error immediately
2. Identify the failure point in the process
3. Implement corrective measures
4. Document the error pattern for future prevention
5. Update verification protocols accordingly

def systematic_document_analysis(document, query):
    # Phase 1: Setup
    search_criteria = define_search_criteria(query)
    
    # Phase 2: Structured Reading
    structure_map = map_document_structure(document)
    target_sections = identify_likely_sections(structure_map, search_criteria)
    candidates = deep_scan_sections(target_sections, search_criteria)
    
    # Phase 3: Verification
    verified_results = verify_candidates(candidates, search_criteria)
    
    # Phase 4: Temporal Consistency
    secondary_results = independent_verification(document, query)
    final_results = consistency_check(verified_results, secondary_results)
    
    # Phase 5: Quality Control
    confidence_score = calculate_confidence(final_results)
    
    return final_results, confidence_score