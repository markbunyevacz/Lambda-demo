---
description: 
globs: 
alwaysApply: false
---
## 1. PROJEKT √ÅTTEKINT√âS

### 1.1 Minimum Credible Product (MCP) C√©lja
**üéØ C√©l:** Bemutatni a rendszer alapvet≈ë k√©pess√©g√©t: egy felhaszn√°l√≥ term√©szetes nyelven kereshet √©p√≠t≈ëanyagot, a rendszer pedig a naprak√©sz, "scraped" adatok alapj√°n relev√°ns term√©keket √©s AI-alap√∫ szak√©rt≈ëi v√°laszt ad.

### 1.2 MCP Scope
**‚úÖ R√©sze az MCP-nek:**
- **Gy√°rt√≥k**: Csak 1 gy√°rt√≥ (Rockwool) adatainak teljes feldolgoz√°sa
- **Hibrid adatgy≈±jt√©s**: Hagyom√°nyos scraper + **üöÄ BrightData MCP AI** (48 tools, CAPTCHA solving)
- **Adatb√°zisok**: Struktur√°lt adatb√°zis (PostgreSQL) √©s vektor adatb√°zis (Chroma)
- **Keres√©s**: Term√©szetes nyelv≈± keres√©s (RAG pipeline) √©s egyszer≈± term√©klista
- **UI**: Egy keres≈ëmez≈ë, egy AI chat ablak √©s egy term√©klista megjelen√≠t≈ë

**‚ùå Nem r√©sze az MCP-nek:**
- T√∂bb gy√°rt√≥ t√°mogat√°sa
- Aj√°nlatk√©sz√≠t≈ë modul
- Komplex sz≈±r≈ëk
- Felhaszn√°l√≥i fi√≥kok

---

## 2. AKTU√ÅLIS ST√ÅTUSZ

### 2.1 ‚úÖ PRODUCTION COMPLETE (Verified & Tested)
**Total: 4 major modules completed to production standard**

#### **Rockwool Live Scraping System** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: 57 files downloaded with 100% success rate (45 datasheets + 12 brochures)
- **Tested**: Live data scraping with zero fallback dependency
- **Location**: `src/backend/src/downloads/rockwool_datasheets/` (34 unique + 11 duplicates + 12 brochures)
- **Features**: 
  - **Live-only data fetching** - No fallback to debug files
  - **Smart duplicate handling** with hash-based unique naming
  - **Hungarian character support** with HTML entity decoding
  - **Concurrent async downloads** for optimal performance
- **Performance**: 45 datasheets + 12 brochures, 57/57 successful, 0 failed
- **Architecture**: Two specialized scrapers working in harmony
  - `rockwool_product_scraper.py` - Product datasheets (live JSON parsing)
  - `brochure_and_pricelist_scraper.py` - Marketing materials (live HTML parsing)

#### **Rockwool State Management System** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: Comprehensive state preservation across JSON, CSV, and SQLite formats
- **Tested**: `RockwoolStateManager` with full lifecycle management
- **Location**: `src/backend/src/rockwool_states/` with organized subdirectories
- **Features**:
  - **Multi-format export**: JSON (complete state), CSV (products), SQLite (queryable database)
  - **Automatic state capture** after each scraping session
  - **Version control** with timestamp-based state IDs
  - **Product tracking** with unique hash identifiers
  - **Statistics preservation** (downloads, duplicates, performance metrics)
  - **Configuration tracking** (scraper settings, data sources)
- **Storage Structure**:
  ```
  src/backend/src/rockwool_states/
  ‚îú‚îÄ‚îÄ rockwool_YYYYMMDD_HHMMSS_complete.json    # Full state
  ‚îú‚îÄ‚îÄ exports/rockwool_YYYYMMDD_HHMMSS_products.csv  # CSV export
  ‚îú‚îÄ‚îÄ rockwool_states.db                        # SQLite database
  ‚îî‚îÄ‚îÄ snapshots/                               # Point-in-time backups
  ```
- **Integration**: Seamlessly integrated into both scrapers for automatic state preservation

#### **Database Integration System** ‚úÖ PRODUCTION COMPLETE
- **Evidence**: 46 ROCKWOOL products successfully integrated into PostgreSQL
- **Tested**: `demo_database_integration.py` - Full end-to-end production testing completed (2025-06-30)
- **Database**: PostgreSQL running in Docker with complete product catalog
- **Features**: Real manufacturer (ROCKWOOL), 6 product categories, smart categorization
- **API**: Live database accessible via FastAPI at `http://localhost:8000/products`
- **Performance**: 46/46 products created successfully, 0 failures, real-time verification completed

### 2.2 ‚úÖ INFRASTRUCTURE READY (Awaiting Verification)
**Total: 42 components with infrastructure completed**

#### **Alapinfrastrukt√∫ra** ‚úÖ
- Projektstrukt√∫ra fel√°ll√≠t√°sa (Backend, Docker)
- Docker-compose konfigur√°ci√≥ (FastAPI, PostgreSQL, Redis)
- Adatb√°zis s√©m√°k √©s SQLAlchemy modellek
- Alapvet≈ë FastAPI alkalmaz√°s (CORS √©s DB kapcsolattal)
- Alapvet≈ë Next.js/React frontend v√°z

#### **BrightData MCP AI Scraping Rendszer** ‚úÖ INFRASTRUCTURE READY (Strategic Reserve)
- **Evidence**: Connection to MCP server successful, all 48 tools loaded
- **Tested**: Standalone scripts confirmed MCP server initialization
- **Status**: **Strategic reserve for complex sites** - Not needed for current Rockwool implementation
- **Performance Analysis**: 
  - **BrightData MCP**: ~5-10 seconds/page, complex setup, $500+/month
  - **Direct HTTP**: ~1.5 seconds/page, simple setup, free
  - **Rockwool Assessment**: No CAPTCHA, no JavaScript dependency, JSON data available
- **Recommendation**: Keep available for future complex sites (Leier, Baumit if needed)
- **Requires**: Human verification only when encountering protected sites

#### **Adatb√°zis √©s Automatiz√°l√°s** ‚úÖ INFRASTRUCTURE READY
- **Adatb√°zis Integr√°ci√≥s Logika**: All data models and integration services defined
- **Celery √©s Celery Beat**: All tasks and scheduling configurations defined
- **Integr√°ci√≥s Tesztel√©s**: `test_integration.py` framework exists

### 2.3 üîÑ IN DEVELOPMENT (Current Priority)
**Immediate Next Steps:**
1. **Client-Specific Architecture** - Modular design implementation for multiple manufacturers
2. **Factory Pattern Implementation** - Reusable scraper framework development
3. **RAG Pipeline Foundation** - Chroma vector database initialization with existing data

### 2.4 üîç AWAITING HUMAN VERIFICATION
**Critical Items Requiring Testing:**
1. **BrightData MCP Integration** - Production testing required for full AI-driven scraping
2. **Celery Automation** - Production scheduling verification for automated tasks

---

## 3. FEJLESZT√âSI F√ÅZISOK

### 3.1 F√°zis 1: Alapoz√°s √©s Infrastrukt√∫ra ‚úÖ VERIFIED COMPLETE
- [x] Projektstrukt√∫ra fel√°ll√≠t√°sa (Backend, Docker)
- [x] Docker-compose konfigur√°ci√≥ (FastAPI, PostgreSQL, Redis)
- [x] Adatb√°zis s√©m√°k √©s SQLAlchemy modellek l√©trehoz√°sa
- [x] Alapvet≈ë FastAPI alkalmaz√°s l√©trehoz√°sa, CORS √©s DB kapcsolattal
- [x] Alapvet≈ë Next.js/React frontend v√°z l√©trehoz√°sa

### 3.2 F√°zis 2: Adat-pipeline √©s Web Scraping ‚úÖ PRODUCTION COMPLETE & VERIFIED
- [x] **Rockwool Live Scraping System** ‚úÖ PRODUCTION COMPLETE (Live-only, 57 files)
- [x] **Rockwool State Management System** ‚úÖ PRODUCTION COMPLETE (Multi-format preservation)
- [x] **BrightData MCP AI Scraping Rendszer** ‚úÖ STRATEGIC RESERVE (48 tools, ready when needed)
- [x] **Adatb√°zis Integr√°ci√≥s Logika** ‚úÖ PRODUCTION COMPLETE (PostgreSQL + ChromaDB)
- [x] **Celery √©s Celery Beat Id≈ëz√≠t√©s** ‚úÖ INFRASTRUCTURE READY
- [x] **Integr√°ci√≥s Tesztel√©s** ‚úÖ VERIFIED (RAG search working)
- [ ] **Leier scraper implement√°ci√≥ja** üîÑ PLANNED (BrightData MCP candidate)
- [ ] **Baumit scraper implement√°ci√≥ja** üîÑ PLANNED (Assessment needed)

### 3.3 F√°zis 3: AI Modul - RAG Pipeline üîÑ IN DEVELOPMENT
#### **Adat-√∂kosziszt√©ma √©s Unifik√°ci√≥ (Data Ecosystem & Unification)**
- [ ] **Adat-aggreg√°ci√≥s szolg√°ltat√°s l√©trehoz√°sa** a k√ºl√∂nb√∂z≈ë PDF-ekb≈ël (adatlap, √°rlista, seg√©dlet) sz√°rmaz√≥ extrakci√≥s eredm√©nyek egyes√≠t√©s√©re. `üîÑ PLANNED`
- [ ] **"Golden Record" s√©ma √©s √∂sszerendel√©si logika** implement√°l√°sa, amely egyedi term√©kazonos√≠t√≥ (pl. term√©kk√≥d) alapj√°n k√∂ti √∂ssze a dokumentumokat. `üîÑ PLANNED`
- [ ] **PDF-ek k√∂z√∂tti √°r-integr√°ci√≥ megval√≥s√≠t√°sa**, ahol a term√©kadatlapokhoz automatikusan hozz√°rendel≈ëdnek az √°rlist√°kb√≥l sz√°rmaz√≥ √°rak. `üîÑ PLANNED`
- [ ] **Intelligens ChromaDB bet√∂lt≈ë (Ingestion) fejleszt√©se**, amely a "Golden Record"-ot haszn√°lja fel gazdag, sz≈±rhet≈ë metaadatok (√°r, m≈±szaki adatok) l√©trehoz√°s√°ra a vektorokhoz. `üîÑ PLANNED`
- [ ] **Egys√©ges√≠tett adat-pipeline monitoring fel√ºlet** alapjainak kialak√≠t√°sa a feldolgoz√°s sikeress√©g√©nek √©s az adatintegrit√°snak a nyomon k√∂vet√©s√©re. `üîÑ PLANNED`

#### **Hibrid Vektor Adatb√°zis**
- [x] **Hibrid Chroma vektor adatb√°zis** inicializ√°l√°sa √©s perziszt√°l√°sa ‚úÖ COMPLETE
- [x] Hagyom√°nyos scraped adatok vektoriz√°l√°sa ‚úÖ COMPLETE (92 products)
- [ ] **üöÄ AI-enhanced term√©kle√≠r√°sok** indexel√©se BrightData MCP eredm√©nyekb≈ël
- [ ] Magyar nyelvi embeddings optimaliz√°l√°s

#### **LangChain Integr√°ci√≥**
- [ ] **LangChain integr√°ci√≥ tov√°bbfejlesztve** √©s `BuildingMaterialsAI` service l√©trehoz√°sa
- [ ] **AI scraping context** integr√°l√°sa a RAG pipeline-ba
- [ ] **Intelligent retrieval** - forr√°s preferenci√°k (API vs MCP AI)
- [ ] **Real-time scraping capability** term√©szetes nyelv≈± k√©r√©sekre

#### **AI K√©pess√©gek**
- [ ] Term√©kadatok automatikus vektoriz√°l√°sa √©s indexel√©se (hibrid forr√°sok)
- [ ] Q&A l√°nc l√©trehoz√°sa a magyar nyelv≈±, √©p√≠t√©szeti szak√©rt≈ëi prompt-tal
- [ ] **üöÄ AI confidence score** alap√∫ term√©k rangsorol√°s √©s v√°laszmin≈ës√©g
- [ ] Kompatibilit√°s-ellen≈ërz≈ë logika alapjainak implement√°l√°sa (`check_system_compatibility`)

### 3.4 F√°zis 4: Backend API √©s Frontend Integr√°ci√≥ üîÑ PLANNED
#### **Hibrid API V√©gpontok**
- [ ] **Hibrid term√©kkeres≈ë API** v√©gpont l√©trehoz√°sa (sz√∂veges keres√©s + sz≈±r√©s)
- [ ] **üöÄ `/api/search/hybrid`** - Kombin√°lja hagyom√°nyos + AI eredm√©nyeket
- [ ] **üöÄ `/api/scraping/intelligent`** - Real-time AI scraping k√©r√©sre
- [ ] **üöÄ `/api/scraping/status`** - AI scraping feladatok monitoring

#### **AI Chat API**
- [ ] **AI Chat API tov√°bbfejlesztve** (`get_product_recommendations`)
- [ ] **üöÄ `/api/ai/analyze-product`** - Term√©k AI elemz√©s k√©r√©sre
- [ ] BrightData MCP eredm√©nyek integr√°l√°sa v√°laszokba
- [ ] Kompatibilit√°s-ellen≈ërz≈ë API v√©gpont l√©trehoz√°sa (AI-enhanced)

#### **Frontend Fejleszt√©sek**
- [ ] **Frontend: AI-enhanced term√©kkeres≈ë** interface fejleszt√©se
- [ ] **üöÄ Real-time scraping status** indicator
- [ ] **üöÄ AI confidence score** megjelen√≠t√©s term√©kekn√©l
- [ ] **üöÄ "K√©rd el AI-t√≥l √∫j term√©keket"** funkci√≥
- [ ] Frontend: AI Chat komponens fejleszt√©se √©s bek√∂t√©se
- [ ] Frontend: Term√©k adatlap √©s √∂sszehasonl√≠t√≥ komponens fejleszt√©se
- [ ] **üöÄ Scraping admin panel** - strat√©gia v√°lt√°s, monitoring (fejleszt≈ëknek)

### 3.5 F√°zis 5: Aj√°nlatk√©sz√≠t≈ë Modul üîÑ PLANNED
- [ ] `quotes` adatb√°zis t√°bla √©s SQLAlchemy modell
- [ ] `QuoteCalculator` service implement√°l√°sa (√°rkalkul√°ci√≥, vesztes√©gsz√°m√≠t√°s)
- [ ] Aj√°nlatk√©sz√≠t≈ë API v√©gpontok (l√©trehoz√°s, lek√©rdez√©s)
- [ ] PDF gener√°l√≥ modul integr√°l√°sa (ReportLab)
- [ ] Email k√ºld≈ë szolg√°ltat√°s integr√°ci√≥ja
- [ ] Frontend: Aj√°nlatk√©sz√≠t≈ë UI (kos√°r, v√©gleges√≠t√©s)

### 3.6 F√°zis 6: Finaliz√°l√°s √©s Deployment üîÑ PLANNED
- [ ] Teljes k√∂r≈± API √©s frontend tesztel√©s (pytest, Jest/Playwright)
- [ ] `Dockerfile`-ok finomhangol√°sa √©les k√∂rnyezetre
- [ ] CI/CD pipeline alapjainak l√©trehoz√°sa (pl. GitHub Actions)
- [ ] R√©szletes dokument√°ci√≥ (API Swagger, README)
- [ ] Felhaszn√°l√≥i tesztel√©s √©s visszajelz√©sek feldolgoz√°sa

---

## 4. AI AGENT FEJLESZT√âSEK

### 4.1 ü§ñ AI Agent Implement√°ci√≥s Backlog

#### **Adatfeldolgoz√≥ Agent** (`DataProcessingAgent`)
- [ ] Raw adatok normaliz√°l√°sa √©s tiszt√≠t√°sa
- [ ] Duplik√°tumok intelligens elt√°vol√≠t√°sa
- [ ] Kategoriz√°l√°s √©s c√≠mk√©z√©s automatiz√°l√°sa
- [ ] Min≈ës√©gbiztos√≠t√°si pipeline implement√°l√°sa

#### **Aj√°nl√°si Agent - RAG** (`RecommendationAgent`)
- [ ] Term√©k √∂sszehasonl√≠t√°si logika
- [ ] Szem√©lyre szabott aj√°nl√°sok gener√°l√°sa
- [ ] RAG alap√∫ v√°laszgener√°l√°s implement√°l√°sa
- [ ] Kontextus meg√©rt√©s √©s mem√≥ria kezel√©s

#### **√Årfigyel≈ë Agent** (`PriceMonitoringAgent`)
- [ ] √År tracking k√ºl√∂nb√∂z≈ë forr√°sokon kereszt√ºl
- [ ] Trend anal√≠zis √©s el≈ërejelz√©s
- [ ] Riaszt√°sok gener√°l√°sa √°r v√°ltoz√°sokra
- [ ] Historikus adatok kezel√©se √©s archiv√°l√°sa

#### **Kompatibilit√°si Agent** (`CompatibilityAgent`)
- [ ] Term√©kek kompatibilit√°s√°nak automatikus ellen≈ërz√©se
- [ ] M≈±szaki specifik√°ci√≥k √∂sszehasonl√≠t√°sa
- [ ] Alkalmaz√°si ter√ºletek elemz√©se
- [ ] Szabv√°nyok √©s el≈ë√≠r√°sok ellen≈ërz√©se

### 4.2 üèóÔ∏è Agent Infrastrukt√∫ra Fejleszt√©sek

#### **Event Bus Integr√°ci√≥** (`AgentEventBus`)
- [ ] Agent-to-agent kommunik√°ci√≥s rendszer
- [ ] Event subscription/publishing mechanizmus
- [ ] Message queuing (Redis) integr√°ci√≥
- [ ] Error handling √©s retry logika

#### **Agent Health Monitoring** (`AgentHealthMonitor`)
- [ ] Heartbeat ellen≈ërz√©si rendszer
- [ ] Performance metrik√°k gy≈±jt√©se
- [ ] Resource haszn√°lat tracking
- [ ] Automatikus √∫jraind√≠t√°si mechanizmus

#### **Agent Lifecycle Management**
- [ ] State management (INITIALIZING ‚Üí IDLE ‚Üí WORKING ‚Üí ERROR/STOPPING ‚Üí STOPPED)
- [ ] Graceful shutdown √©s restart
- [ ] Configuration hot-reload
- [ ] Service discovery implement√°l√°sa

#### **Performance Metrik√°k Rendszer** (`AgentMetrics`)
- [ ] Task completion rate tracking
- [ ] Average response time m√©r√©se
- [ ] Error rate monitoring
- [ ] Resource consumption analysis
- [ ] System-wide metrik√°k (total active agents, message throughput)

### 4.3 üõ†Ô∏è Fejleszt≈ëi Tooling √©s Template-ek

#### **Agent Template Script** implement√°l√°sa
- [ ] `python scripts/create_agent.py --name MyNewAgent --type data_processing`
- [ ] Skeleton k√≥dgener√°l√°s
- [ ] Konfigur√°ci√≥ template-ek
- [ ] Testing boilerplate gener√°l√°sa

#### **Integration Checklist** automatiz√°l√°sa
- [ ] Agent oszt√°ly implement√°l√°s ellen≈ërz√©se
- [ ] Health check endpoint valid√°l√°sa
- [ ] Metrics collection tesztel√©se
- [ ] Unit √©s integration tesztek futtat√°sa

#### **Migration Protocol** implement√°l√°sa
- [ ] Client-specific architecture √°t√°ll√°si l√©p√©sek
- [ ] Modular scraper design pattern alkalmaz√°sa
- [ ] Factory pattern implement√°l√°sa (`ClientFactory.create_scraper()`)
- [ ] Clean API interface l√©trehoz√°sa

---

## 5. K√ñVETKEZ≈ê L√âP√âSEK

### 5.1 üéØ Azonnal Megval√≥s√≠tand√≥ (H√©t 1-2)
1. **RAG Pipeline Foundation** - Chroma vector database initialization with existing 46 products
2. **BrightData MCP Production Testing** - Full AI-driven scraping task verification
3. **Client-Specific Architecture** - Modular design implementation for multiple manufacturers

### 5.2 üîÑ R√∂vid T√°v√∫ (H√©t 3-4)
1. **Factory Pattern Implementation** - Reusable scraper framework
2. **Celery Automation Testing** - Production scheduling verification
3. **RAG Pipeline Foundation** - Chroma vector database initialization

### 5.3 üìã K√∂zepes T√°v√∫ (H√©t 5-8)
1. **AI Agent Infrastructure** - Event Bus, Health Monitoring
2. **Hibrid API Endpoints** - Intelligent search and scraping
3. **Frontend AI Integration** - Real-time status, confidence scores

---

## 6. PROVEN METODOL√ìGIA

### 6.1 üéØ SUCCESS PATTERN IDENTIFIED & EVOLVED
1. **Live-First Approach** - Prioritize live data over cached/debug files for production
2. **Evidence-First Development** - Start with existing working data for development
3. **Incremental Testing** - Test each component individually before integration
4. **Zero Data Loss** - Implement duplicate handling, never overwrite data
5. **Smart Duplicate Management** - Hash-based unique naming in dedicated subdirectories
6. **State Preservation** - Comprehensive state management across multiple formats
7. **Performance Optimization** - Choose simplest effective method (HTTP vs AI tools)
8. **Production Validation** - Complete end-to-end testing before marking complete

### 6.2 üîë Key Insights
**Working Code ‚â† Infrastructure Existence**
- Only mark items as PRODUCTION COMPLETE after successful end-to-end testing
- Use "INFRASTRUCTURE READY" for components that exist but lack verification
- Prioritize functional validation over feature expansion

### 6.3 üìä St√°tusz Kateg√≥ri√°k
- **‚úÖ PRODUCTION COMPLETE**: Verified & Tested, ready for production use
- **‚úÖ INFRASTRUCTURE READY**: Code exists, basic tests pass, awaiting verification
- **‚úÖ STRATEGIC RESERVE**: Ready for use when specific conditions are met
- **üîÑ IN DEVELOPMENT**: Active development in progress
- **üîç AWAITING VERIFICATION**: Human testing required
- **üîÑ PLANNED**: Design/requirements ready, not yet started

### 6.4 ü§ñ BrightData MCP Strategic Decision Framework
**When to Use BrightData MCP:**
- ‚úÖ CAPTCHA/reCAPTCHA protection detected
- ‚úÖ JavaScript-heavy SPA with no accessible JSON/API
- ‚úÖ Anti-bot detection (IP blocking, user-agent filtering)
- ‚úÖ Complex authentication flows
- ‚úÖ Geo-restricted content

**When to Use Direct HTTP:**
- ‚úÖ Static HTML content available
- ‚úÖ JSON/API endpoints accessible
- ‚úÖ No anti-bot protection
- ‚úÖ Simple form-based interactions
- ‚úÖ Performance and cost optimization priority

**Rockwool Case Study:**
- **Assessment**: Static JSON data, no protection, simple structure
- **Decision**: Direct HTTP (5-10x faster, cost-free)
- **Result**: 57/57 files downloaded successfully in ~4-6 minutes
- **BrightData Status**: Strategic reserve for future complex sites

---

---

## üìã DOKUMENT√ÅCI√ì HIVATKOZ√ÅSOK

### Kapcsol√≥d√≥ Dokumentumok
- **üìä Kock√°zatelemz√©s √©s Megold√°sok**: [`docs/PDF_PROCESSING_RISK_ANALYSIS_AND_MITIGATION.md`](../docs/PDF_PROCESSING_RISK_ANALYSIS_AND_MITIGATION.md)
- **üèóÔ∏è Technikai Architekt√∫ra**: [`docs/ADAPTIVE_PDF_EXTRACTION_ARCHITECTURE.md`](../docs/ADAPTIVE_PDF_EXTRACTION_ARCHITECTURE.md)
- **üîó MCP Integr√°ci√≥**: [`src/backend/app/mcp_orchestrator/INTEGRATION_STATUS.md`](../src/backend/app/mcp_orchestrator/INTEGRATION_STATUS.md)
- **‚ö†Ô∏è Implementation Constraints**: [`.cursorrules/IMPLEMENTATION_PROCESS_AND_CONSTRAINTS.mdc`](IMPLEMENTATION_PROCESS_AND_CONSTRAINTS.mdc)

---

## üìö GYORS REFERENCIA

### Legfontosabb St√°tuszok
- **Production Complete**: 4 modulok (Rockwool Live System + State Management + Database Integration)
- **Strategic Reserve**: 1 modul (BrightData MCP - ready when needed)
- **Infrastructure Ready**: 41 komponens
- **Awaiting Verification**: 1 kritikus elem (Celery automation)
- **Planned**: 22+ komponens

### K√∂vetkez≈ë Milestone
**üéØ RAG Pipeline Foundation** - 57 ROCKWOOL file vektoriz√°l√°sa √©s Chroma adatb√°zis inicializ√°l√°sa

### State Management Benefits
- **Data Preservation**: Complete scraping history in multiple formats
- **Version Control**: Timestamp-based state tracking
- **Analytics**: Performance metrics and trend analysis
- **Recovery**: Ability to restore previous states
- **Export**: CSV for analysis, SQLite for querying, JSON for backup

VERIFICATION CHECKLIST:
‚ñ° Did I scan the entire document systematically?
‚ñ° Did I check all major sections and subsections?
‚ñ° Am I confusing similar but different items?
‚ñ° Have I verified my findings against the original request?
‚ñ° If I found nothing, did I complete a full document sweep?

CONSISTENCY PROTOCOL:
1. Initial reading and conclusion
2. Wait/pause briefly
3. Re-approach the same question independently
4. Compare both conclusions
5. If inconsistent, perform tie-breaker third reading

WHEN ERRORS OCCUR:
1. Acknowledge the error immediately
2. Identify the failure point in the process
3. Implement corrective measures
4. Document the error pattern for future prevention
5. Update verification protocols accordingly

def systematic_document_analysis(document, query):
    # Phase 1: Setup
    search_criteria = define_search_criteria(query)
    
    # Phase 2: Structured Reading
    structure_map = map_document_structure(document)
    target_sections = identify_likely_sections(structure_map, search_criteria)
    candidates = deep_scan_sections(target_sections, search_criteria)
    
    # Phase 3: Verification
    verified_results = verify_candidates(candidates, search_criteria)
    
    # Phase 4: Temporal Consistency
    secondary_results = independent_verification(document, query)
    final_results = consistency_check(verified_results, secondary_results)
    
    # Phase 5: Quality Control
    confidence_score = calculate_confidence(final_results)
    
    return final_results, confidence_score

# MANUFACTURER-SPECIFIC CONFIGS (NEVER CROSS)
SCRAPER_CONFIGS = {
    'ROCKWOOL': {
        'base_url': 'https://www.rockwool.com',
        'storage_path': 'rockwool_datasheets',
        'endpoints': [
            '/hu/muszaki-informaciok/termekadatlapok/',
            '/hu/muszaki-informaciok/arlistak-es-prospektusok/'
        ]
    },
    'LEIER': {
        'base_url': 'https://www.leier.hu', 
        'storage_path': 'leier_products',
        'endpoints': [
            '/hu/termekek/',
            '/hu/letoltheto-dokumentumok/',
            '/hu/tipushazak/'
        ]
    },
    'BAUMIT': {
        'base_url': 'https://baumit.hu',
        'storage_path': 'baumit_products', 
        'endpoints': [
            '/termekek-a-z',          # ‚úÖ IMPLEMENTED: A-Z product catalog
            '/baumit-life',           # üîí IMPLEMENTED: Color system scraper
            '/baumit-katalogus/',     # üîú TODO: Product catalogs
            '/life-szinrendszer/'     # üîú TODO: Color system details
        ]
    }
}

class IsolatedScraperFactory:
    @staticmethod
    def create_scraper(manufacturer: str):
        config = SCRAPER_CONFIGS[manufacturer.upper()]
        
        if manufacturer == 'ROCKWOOL':
            return RockwoolScraper(config)  # ‚úÖ Existing
        elif manufacturer == 'LEIER':
            return LeierScraper(config)     # üîí New isolated  
        elif manufacturer == 'BAUMIT':
            return BaumitScraper(config)    # ‚úÖ IMPLEMENTED: Isolated BAUMIT scrapers
        else:
            raise ValueError(f"Unknown manufacturer: {manufacturer}")

class RockwoolCategoryMapper(CategoryMapper):
    """ROCKWOOL categories only"""
    def __init__(self):
        self.manufacturer = "ROCKWOOL"
        self.category_mappings = {
            'Tet≈ëszigetel√©s': {...},
            'Homlokzati h≈ëszigetel√©s': {...},
            # ... ROCKWOOL specific
        }

class LeierCategoryMapper(CategoryMapper):
    """LEIER categories only"""  
    def __init__(self):
        self.manufacturer = "LEIER"
        self.category_mappings = {
            'Falaz√≥elemek': {...},
            'El≈ëregy√°rtott elemek': {...},
            # ... LEIER specific  
        }

class BaumitCategoryMapper(CategoryMapper):
    """BAUMIT categories only - ‚úÖ IMPLEMENTED"""
    def __init__(self):
        self.manufacturer = "BAUMIT"
        self.category_mappings = {
            'H≈ëszigetel≈ë rendszerek': 'Thermal Insulation Systems',
            'Homlokzatfest√©kek': 'Fa√ßade Paints',
            'Sz√≠nes v√©konyvakolatok': 'Colored Thin-layer Renders',
            'Aljzatk√©pz≈ë ragaszt√≥ rendszerek': 'Substrate Adhesive Systems',
            'Homlokzati fel√∫j√≠t√≥ rendszerek': 'Fa√ßade Renovation Systems',
            'Belt√©ri vakolatok': 'Interior Renders',
            'Glettek √©s fest√©kek': 'Fillers and Paints',
            'Baumit Life sz√≠nrendszer': 'Baumit Life Color System'
            # ‚úÖ FULL IMPLEMENTATION with 8 main categories
        }

# FACTORY ENSURES NO CROSS-CONTAMINATION
def get_category_mapper(manufacturer: str):
    mappers = {
        'ROCKWOOL': RockwoolCategoryMapper(),
        'LEIER': LeierCategoryMapper(), 
        'BAUMIT': BaumitCategoryMapper()
    }
    return mappers[manufacturer.upper()]

class IsolatedScrapingOrchestrator:
    def run_manufacturer_scraping(self, manufacturer: str):
        """Each manufacturer runs in complete isolation"""
        
        # 1. Isolated configuration
        config = SCRAPER_CONFIGS[manufacturer]
        
        # 2. Isolated storage
        storage_path = get_storage_path(manufacturer)
        
        # 3. Isolated database integration  
        db_integration = ManufacturerIsolatedDatabaseIntegration(manufacturer)
        
        # 4. Isolated category mapping
        category_mapper = get_category_mapper(manufacturer)
        
        # 5. Run scraper with isolated context
        scraper = IsolatedScraperFactory.create_scraper(manufacturer)
        scraper.run()
        
        # IMPOSSIBLE to affect other manufacturers

# docker-compose.yml - Service isolation
services:
  rockwool-scraper:     # ‚úÖ Existing
    volumes:
      - ./src/downloads/rockwool_datasheets:/app/downloads/rockwool_datasheets
    environment:
      - MANUFACTURER=ROCKWOOL
      
  leier-scraper:        # üîí New isolated
    volumes:  
      - ./src/downloads/leier_products:/app/downloads/leier_products
    environment:
      - MANUFACTURER=LEIER
      
  baumit-scraper:       # ‚úÖ IMPLEMENTED: Isolated BAUMIT service
    volumes:
      - ./src/downloads/baumit_products:/app/downloads/baumit_products  
    environment:
      - MANUFACTURER=BAUMIT
      - BAUMIT_BASE_URL=https://baumit.hu
      - BAUMIT_CATALOG_PATH=/termekek-a-z

class IsolatedErrorHandler:
    def handle_scraper_error(self, manufacturer: str, error: Exception):
        """Errors in one manufacturer NEVER affect others"""
        
        error_log_path = get_storage_path(manufacturer) / "errors.log"
        
        with open(error_log_path, 'a') as f:
            f.write(f"{datetime.now()}: {manufacturer} error: {error}\n")
        
        # Report to manufacturer-specific monitoring
        self._notify_manufacturer_team(manufacturer, error)
        
        # ISOLATION: Other manufacturers continue unaffected
        logger.warning(f"{manufacturer} scraper failed, others continue")

class TestRockwoolIsolation(unittest.TestCase):
    """Verify ROCKWOOL data remains untouched"""
    
    def test_rockwool_data_unchanged(self):
        # Verify 129 ROCKWOOL products still exist
        rockwool_count = db.query(Product).filter(
            Product.manufacturer.has(name='ROCKWOOL')
        ).count()
        self.assertEqual(rockwool_count, 129)
        
    def test_rockwool_files_unchanged(self):
        # Verify all 41 PDF files still exist
        rockwool_files = list(Path("src/downloads/rockwool_datasheets").glob("*.pdf"))
        self.assertEqual(len(rockwool_files), 41)

class TestLeierIsolation(unittest.TestCase):
    """Verify LEIER has its own isolated space"""
    
    def test_leier_directory_isolation(self):
        leier_path = Path("src/downloads/leier_products")
        self.assertTrue(leier_path.exists())
        self.assertNotEqual(leier_path, Path("src/downloads/rockwool_datasheets"))

class IsolationMonitor:
    def validate_isolation(self):
        """Continuous validation that no cross-contamination occurs"""
        
        # 1. Database isolation check
        manufacturers = db.query(Manufacturer).all()
        for mfr in manufacturers:
            products = db.query(Product).filter(
                Product.manufacturer_id == mfr.id
            ).all()
            
            # Verify all products belong to correct manufacturer
            for product in products:
                assert product.manufacturer_id == mfr.id
                
        # 2. File isolation check  
        for manufacturer, path in MANUFACTURER_STORAGE.items():
            if path.exists():
                files = list(path.glob("**/*"))
                # Verify no cross-manufacturer files
                for file in files:
                    assert manufacturer.lower() in str(file).lower()
                    
        # 3. SKU isolation check
        all_skus = db.query(Product.sku).all()
        sku_prefixes = {'ROCK': 'ROCKWOOL', 'LEIR': 'LEIER', 'BAUM': 'BAUMIT'}
        
        for (sku,) in all_skus:
            if sku:
                prefix = sku.split('-')[0]
                expected_manufacturer = sku_prefixes[prefix]
                actual_manufacturer = db.query(Product).filter(
                    Product.sku == sku
                ).first().manufacturer.name
                assert actual_manufacturer == expected_manufacturer

# High Priority: A-Z Product Catalog
python src/backend/app/scrapers/baumit_final/run_baumit_scraper.py catalog

# Medium Priority: Color Systems  
python src/backend/app/scrapers/baumit_final/run_baumit_scraper.py colors

# All BAUMIT scrapers
python src/backend/app/scrapers/baumit_final/run_baumit_scraper.py all

downloads/baumit_products/
‚îú‚îÄ‚îÄ technical_datasheets/
‚îÇ   ‚îú‚îÄ‚îÄ thermal_insulation/
‚îÇ   ‚îú‚îÄ‚îÄ facade_systems/
‚îÇ   ‚îú‚îÄ‚îÄ adhesive_systems/
‚îÇ   ‚îî‚îÄ‚îÄ interior_solutions/
‚îú‚îÄ‚îÄ color_systems/
‚îÇ   ‚îú‚îÄ‚îÄ baumit_life_colors/
‚îÇ   ‚îî‚îÄ‚îÄ color_charts/
‚îî‚îÄ‚îÄ duplicates/