#!/usr/bin/env python3
"""
Lambda.hu Projekt Takar√≠t√°si Script
===================================

Elt√°vol√≠tja az ideiglenes f√°jlokat, tesztscripteket, debug f√°jlokat,
placeholder tartalmakat √©s szimul√°ci√≥ scripteket.

Kateg√≥ri√°k:
- Debug/teszt scriptek
- Extract/analyze scriptek  
- Show/list scriptek
- Rebuild/complete scriptek
- JSON report/adatf√°jlok
- Duplik√°lt k√∂nyvt√°rak
- HTML debug f√°jlok
- Egyszer haszn√°lt scriptek
"""

import os
import shutil
from pathlib import Path
from typing import List, Set

class ProjectCleaner:
    def __init__(self, dry_run: bool = True):
        self.dry_run = dry_run
        self.deleted_files: List[str] = []
        self.deleted_dirs: List[str] = []
        self.skipped_files: List[str] = []
        
    def clean_project(self):
        """F≈ëfunkci√≥ - projekt teljes takar√≠t√°sa"""
        print("üßπ Lambda.hu Projekt Takar√≠t√°s")
        print(f"{'üîç DRY RUN MODE' if self.dry_run else 'üö® LIVE DELETION MODE'}")
        print("=" * 50)
        
        # Root k√∂nyvt√°r takar√≠t√°sa
        self._clean_root_directory()
        
        # Backend k√∂nyvt√°r takar√≠t√°sa
        self._clean_backend_directory()
        
        # Duplik√°lt k√∂nyvt√°rak takar√≠t√°sa
        self._clean_duplicate_directories()
        
        # Eredm√©nyek ki√≠r√°sa
        self._print_summary()
    
    def _clean_root_directory(self):
        """Root k√∂nyvt√°r ideiglenes f√°jljainak takar√≠t√°sa"""
        print("\nüìÅ ROOT K√ñNYVT√ÅR TAKAR√çT√ÅSA")
        
        # Debug f√°jlok
        debug_files = [
            "debug_agent.py",
            "debug_env.py", 
            "debug_connect_args.py",
            "debug_live_database.py",
            "debug_database_utf8.py",
            "debug_utf8_position66.py",
            "debug_baumit_catalog.html"
        ]
        
        # Teszt f√°jlok
        test_files = [
            "test_agent.py",
            "test_leier_content.py", 
            "test_database_integration.py"
        ]
        
        # Extract/process scriptek
        extract_files = [
            "extract_pdf_data.py",
            "complete_pdf_extraction.py",
            "fix_pdf_metadata_extraction.py",
            "fix_metadata_extraction_docker.py",
            "fix_scraper_path.py"
        ]
        
        # Show/list/analyze scriptek
        show_files = [
            "show_real_pdf_results.py",
            "show_products.py",
            "create_products_list.py", 
            "simple_chromadb_list.py",
            "chromadb_products_list.py",
            "get_all_chroma_products.py",
            "list_all_chroma_products.py",
            "analyze_products.py"
        ]
        
        # Rebuild scriptek
        rebuild_files = [
            "rebuild_simple.py",
            "rebuild_chromadb_docker.py", 
            "rebuild_chromadb_with_specs.py"
        ]
        
        # Complete/demo/clean scriptek
        complete_files = [
            "complete_solution.py",
            "complete_duplicate_solution.py",
            "demo_database_integration.py",
            "clean_database_duplicates.py",
            "add_database_constraints.py",
            "verify_phase_2_completion.py",
            "database_analysis.py",
            "production_pdf_integration.py",
            "deduplication_manager.py"
        ]
        
        # Adatf√°jlok/reportok
        data_files = [
            "chromadb_products_list.csv",
            "chromadb_products_list.json", 
            "chromadb_products_list.txt",
            "real_pdf_tables_extracted.csv",
            "simple_processing_report_20250707_141058.json",
            "extraction_comparison_report.json",
            "scraper_output.log",
            "project_cleanup_report.txt",
            "rockwool_brightdata_mcp_results.json",
            "rockwool_prod_run.json",
            "rockwool_datasheets_summary.json",
            "rockwool_datasheets_complete.json",
            "termekadatlapok_components.json"
        ]
        
        # √ñsszevon√°s √©s t√∂rl√©s
        all_root_files = (
            debug_files + test_files + extract_files + 
            show_files + rebuild_files + complete_files + data_files
        )
        
        for file_name in all_root_files:
            self._delete_file(file_name)
        
        # K√∂nyvt√°rak t√∂rl√©se
        directories_to_remove = [
            "debug_baumit",
            "leier_scraping_reports",
            "reports",
            "factory",
            "shared"
        ]
        
        for dir_name in directories_to_remove:
            self._delete_directory(dir_name)
    
    def _clean_backend_directory(self):
        """Backend k√∂nyvt√°r ideiglenes f√°jljainak takar√≠t√°sa"""
        print("\nüìÅ BACKEND K√ñNYVT√ÅR TAKAR√çT√ÅSA")
        
        backend_path = Path("src/backend")
        
        # Debug/diagnose f√°jlok
        debug_files = [
            "debug_ai_analyzer.py",
            "diagnose_utf8.py",
            "diagnose_strategies.py"
        ]
        
        # Teszt/verify f√°jlok
        test_files = [
            "test_db.py",
            "test_rag_search.py",
            "test_recommendation_agent.py", 
            "verify_postgresql.py",
            "verify_chromadb_search.py"
        ]
        
        # Extract/show/analyze scriptek
        extract_show_files = [
            "extract_pdf_data.py",
            "show_real_pdf_results.py",
            "show_products.py",
            "create_products_list.py",
            "simple_chromadb_list.py", 
            "chromadb_products_list.py",
            "get_all_chroma_products.py",
            "list_all_chroma_products.py",
            "analyze_products.py"
        ]
        
        # Rebuild/complete scriptek
        rebuild_complete_files = [
            "rebuild_simple.py",
            "rebuild_chromadb_docker.py",
            "rebuild_chromadb_with_focused_data.py",
            "complete_pdf_extraction.py",
            "cleanup_test_data.py"
        ]
        
        # Check f√°jlok
        check_files = [
            "check_airrock_hd_fb1.py",
            "check_postgresql_data.py", 
            "check_chromadb.py"
        ]
        
        # Run scriptek
        run_files = [
            "run_brightdata_mcp.py",
            "run_brightdata_simple.py",
            "run_rag_pipeline_init.py"
        ]
        
        # Sync/fix f√°jlok
        sync_fix_files = [
            "sync_postgresql_to_chromadb.py",
            "fix_metadata_extraction_docker.py"
        ]
        
        # JSON adatf√°jlok/reportok
        json_data_files = [
            "raw_haiku_test_results.json",
            "complete_extraction_demo_results.json", 
            "openapi_debug.json",
            "production_pdf_report.json",
            "chromadb_products_list.txt",
            "chromadb_products_list.json",
            "chromadb_products_complete_list.json",
            "readable_products_report_20250701_113522.txt", 
            "complete_products_data_20250701_113522.json",
            "rockwool_brightdata_mcp_results.json",
            "rockwool_simple_mcp_results.json",
            "real_pdf_extraction_results.json"
        ]
        
        # HTML debug f√°jlok
        html_files = [
            "rockwool_datasheet_page.html"
        ]
        
        # Egyszer haszn√°lt demo f√°jlok
        demo_files = [
            "simple_working_demo.py",
            "production_verification_report.py"
        ]
        
        # Dokument√°ci√≥ (felesleges m√°r)
        doc_files = [
            "DUPLICATE_CLEANUP_REPORT.md",
            "INSTALLATION_LOG.md",
            "README_BRIGHTDATA_MCP.md", 
            "BRIGHTDATA_MCP_COMPLETE_SETUP.md",
            "BRIGHTDATA_MCP_SETUP.md",
            "TROUBLESHOOTING.md"
        ]
        
        # Database f√°jlok
        db_files = [
            "test.db",
            "celerybeat-schedule"
        ]
        
        # √ñsszevon√°s
        all_backend_files = (
            debug_files + test_files + extract_show_files + 
            rebuild_complete_files + check_files + run_files + 
            sync_fix_files + json_data_files + html_files + 
            demo_files + doc_files + db_files
        )
        
        for file_name in all_backend_files:
            self._delete_file(backend_path / file_name)
        
        # Backend k√∂nyvt√°rak t√∂rl√©se
        backend_directories = [
            "leier_scraping_reports",
            "reports", 
            "src",
            "chromadb_data",
            "data",
            "models",
            "processing",
            "tools"
        ]
        
        for dir_name in backend_directories:
            self._delete_directory(backend_path / dir_name)
    
    def _clean_duplicate_directories(self):
        """Duplik√°lt k√∂nyvt√°rak takar√≠t√°sa"""
        print("\nüìÅ DUPLIK√ÅLT K√ñNYVT√ÅRAK TAKAR√çT√ÅSA")
        
        # Root szint≈± duplik√°lt k√∂nyvt√°rak
        duplicate_dirs = [
            "chromadb_data",  # Van src/backend/chromadb_data is
            "__pycache__",
            ".mypy_cache"
        ]
        
        for dir_name in duplicate_dirs:
            self._delete_directory(dir_name)
    
    def _delete_file(self, file_path: Path | str):
        """F√°jl t√∂rl√©se"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            return
            
        if self.dry_run:
            print(f"   üóÇÔ∏è  [DRY RUN] Delete file: {file_path}")
            self.deleted_files.append(str(file_path))
        else:
            try:
                file_path.unlink()
                print(f"   ‚úÖ Deleted file: {file_path}")
                self.deleted_files.append(str(file_path))
            except Exception as e:
                print(f"   ‚ùå Failed to delete {file_path}: {e}")
                self.skipped_files.append(str(file_path))
    
    def _delete_directory(self, dir_path: Path | str):
        """K√∂nyvt√°r t√∂rl√©se"""
        dir_path = Path(dir_path)
        
        if not dir_path.exists() or not dir_path.is_dir():
            return
            
        if self.dry_run:
            print(f"   üìÅ [DRY RUN] Delete directory: {dir_path}")
            self.deleted_dirs.append(str(dir_path))
        else:
            try:
                shutil.rmtree(dir_path)
                print(f"   ‚úÖ Deleted directory: {dir_path}")
                self.deleted_dirs.append(str(dir_path))
            except Exception as e:
                print(f"   ‚ùå Failed to delete {dir_path}: {e}")
                self.skipped_files.append(str(dir_path))
    
    def _print_summary(self):
        """√ñsszefoglal√≥ ki√≠r√°sa"""
        print("\n" + "=" * 50)
        print("üìä TAKAR√çT√ÅSI √ñSSZEFOGLAL√ì")
        print("=" * 50)
        print(f"üìÑ F√°jlok t√∂r√∂lve: {len(self.deleted_files)}")
        print(f"üìÅ K√∂nyvt√°rak t√∂r√∂lve: {len(self.deleted_dirs)}")
        print(f"‚ö†Ô∏è  Sikertelen t√∂rl√©sek: {len(self.skipped_files)}")
        
        if self.dry_run:
            print("\nüîç DRY RUN MODE - Semmi sem lett t√©nylegesen t√∂r√∂lve!")
            print("A t√©nyleges t√∂rl√©shez futtasd: python cleanup_project_files.py --execute")
        else:
            print("\n‚úÖ Takar√≠t√°s befejezve!")
            
        # R√©szletes lista k√©r√©sre
        if len(self.deleted_files) > 0:
            print(f"\nüìù T√∂r√∂lt f√°jlok list√°ja ({len(self.deleted_files)} db):")
            for file_path in sorted(self.deleted_files)[:20]:  # Els≈ë 20
                print(f"   - {file_path}")
            if len(self.deleted_files) > 20:
                print(f"   ... √©s m√©g {len(self.deleted_files) - 20} f√°jl")

def main():
    import sys
    
    # Argumentum ellen≈ërz√©s
    dry_run = True
    if "--execute" in sys.argv:
        dry_run = False
        print("‚ö†Ô∏è  LIVE DELETION MODE - F√°jlok t√©nylegesen t√∂rl≈ëdni fognak!")
        confirmation = input("Biztos vagy benne? (igen/nem): ")
        if confirmation.lower() not in ['igen', 'yes', 'y']:
            print("‚ùå T√∂rl√©s megszak√≠tva.")
            return
    
    cleaner = ProjectCleaner(dry_run=dry_run)
    cleaner.clean_project()

if __name__ == "__main__":
    main() 