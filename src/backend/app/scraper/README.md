# Rockwool Scraper Dokument√°ci√≥

## √Åttekint√©s

A Rockwool Scraper egy teljes k√∂r≈± automatiz√°l√°si rendszer a Rockwool.hu weboldal term√©kadatainak √∂sszegy≈±jt√©s√©re. Ez a dokument√°ci√≥ r√©szletesen bemutatja a scraper haszn√°lat√°t, konfigur√°ci√≥j√°t √©s az automatiz√°l√°si lehet≈ës√©geket.

## üèóÔ∏è Architekt√∫ra

A scraper modul√°ris fel√©p√≠t√©s≈±, n√©gy f≈ë komponenssel:

### 1. **RockwoolScraper** - F≈ë scraper oszt√°ly
- Weboldal strukt√∫ra felt√©rk√©pez√©se
- Term√©kek automatikus felder√≠t√©se  
- Rate limiting √©s hibakezel√©s
- √öjrapr√≥b√°lkoz√°si logika

### 2. **ProductParser** - HTML elemz√©s √©s adatkinyer√©s
- Term√©knevek √©s le√≠r√°sok feldolgoz√°sa
- M≈±szaki specifik√°ci√≥k t√°bl√°zatokb√≥l
- K√©pek √©s dokumentumok URL-jeinek gy≈±jt√©se
- Rockwool specifikus tiszt√≠t√°si szab√°lyok

### 3. **CategoryMapper** - Kateg√≥ri√°k normaliz√°l√°sa
- URL alap√∫ kateg√≥ria meghat√°roz√°s
- Egys√©ges kateg√≥ria strukt√∫ra
- Term√©knevek alap√∫ automatikus kategoriz√°l√°s

### 4. **DataValidator** - Adatmin≈ës√©g biztos√≠t√°s
- K√∂telez≈ë mez≈ëk ellen≈ërz√©se
- Adatform√°tum valid√°l√°s
- Konzisztencia vizsg√°lat

## üöÄ Gyors ind√≠t√°s

### 1. F√ºgg≈ës√©gek telep√≠t√©se

```bash
# Backend k√∂nyvt√°rban
pip install -r requirements.txt
```

### 2. Egyszer≈± scraping teszt

```python
from app.scraper import RockwoolScraper

# Scraper inicializ√°l√°sa
scraper = RockwoolScraper()

# Weboldal strukt√∫ra felt√©rk√©pez√©se
structure = scraper.discover_website_structure()
print(f"Tal√°lt kateg√≥ri√°k: {len(structure['categories'])}")

# Egyetlen term√©k scraping-je
product = scraper._scrape_single_product(
    "https://www.rockwool.hu/termekek/.../frontrock-max-e/"
)
if product:
    print(f"Scraped: {product.name}")
```

### 3. REST API haszn√°lat

```bash
# Scraper √°llapot ellen≈ërz√©se
curl http://localhost:8000/api/scraper/status

# Weboldal strukt√∫ra felt√©rk√©pez√©se
curl http://localhost:8000/api/scraper/website-structure

# Egyetlen term√©k scraping-je
curl -X POST "http://localhost:8000/api/scraper/scrape/single-product" \
     -H "Content-Type: application/json" \
     -d '{"product_url": "https://www.rockwool.hu/..."}'
```

## ‚öôÔ∏è Konfigur√°ci√≥

### ScrapingConfig oszt√°ly

```python
from app.scraper import ScrapingConfig

config = ScrapingConfig(
    base_url="https://www.rockwool.hu",
    max_requests_per_minute=30,    # Rate limiting
    request_delay=2.0,             # K√©sleltet√©s k√©r√©sek k√∂z√∂tt (m√°sodperc)
    timeout=30,                    # HTTP timeout
    max_retries=3,                 # √öjrapr√≥b√°lkoz√°sok sz√°ma
    user_agent="Mozilla/5.0..."    # User-Agent string
)

scraper = RockwoolScraper(config)
```

### Aj√°nlott be√°ll√≠t√°sok

**Gyors scraping (fejleszt√©s/teszt):**
```python
config = ScrapingConfig(
    request_delay=1.0,
    max_requests_per_minute=60
)
```

**√ìvatos scraping (√©les haszn√°lat):**
```python
config = ScrapingConfig(
    request_delay=3.0,
    max_requests_per_minute=20,
    max_retries=5
)
```

## üìã API V√©gpontok

### GET `/api/scraper/status`
Scraper √°llapot lek√©rdez√©se
```json
{
  "is_running": false,
  "scraped_urls": 42,
  "failed_urls": 3,
  "last_activity": "2024-01-15T10:30:00"
}
```

### GET `/api/scraper/website-structure`
Rockwool weboldal strukt√∫r√°j√°nak felt√©rk√©pez√©se
```json
{
  "success": true,
  "structure": {
    "categories": ["https://www.rockwool.hu/homlokzat/...", ...],
    "product_pages": ["https://www.rockwool.hu/termekek/...", ...]
  },
  "discovered_at": "2024-01-15T10:30:00"
}
```

### POST `/api/scraper/scrape/single-product`
Egyetlen term√©k scraping-je
```json
{
  "product_url": "https://www.rockwool.hu/termekek/.../frontrock-max-e/"
}
```

### POST `/api/scraper/scrape/start`
Teljes scraping ind√≠t√°sa h√°tt√©rben
```json
{
  "max_products_per_category": 10,
  "max_categories": 5,
  "test_mode": false,
  "specific_urls": null
}
```

### GET `/api/scraper/health`
Scraper eg√©szs√©g√ºgyi ellen≈ërz√©s

## üîç Weboldal t√©rk√©pez√©s

A scraper automatikusan felt√©rk√©pezi a Rockwool weboldal strukt√∫r√°j√°t:

### 1. Navig√°ci√≥s men√º elemz√©se
- F≈ëmen√º linkek azonos√≠t√°sa
- Term√©k kateg√≥ri√°k felt√©rk√©pez√©se
- Hierarchikus men√º strukt√∫ra

### 2. Kateg√≥ria felder√≠t√©s
A scraper the k√∂vetkez≈ë kateg√≥ri√°kat azonos√≠tja:
- **Tet≈ëszigetel√©s** (roofrock, deltarock)
- **Homlokzati h≈ëszigetel√©s** (frontrock, multirock)
- **Padl√≥szigetel√©s** (steprock)
- **V√°laszfal szigetel√©s** (airrock)
- **G√©p√©szeti szigetel√©s** (wired mat)
- **T≈±zv√©delem** (hardrock, firesafe)
- **Hangszigetel√©s** (rocksilence)

### 3. Term√©koldal azonos√≠t√°s
- URL pattern alap√∫ felismer√©s
- Term√©knevek alapj√°n
- Breadcrumb navig√°ci√≥ elemz√©se

## üìä Adatkinyer√©s r√©szletei

### Term√©kadatok strukt√∫ra

```python
@dataclass
class ScrapedProduct:
    name: str                    # Term√©k neve (tiszt√≠tva)
    url: str                     # Term√©k URL-je
    category: str                # Normaliz√°lt kateg√≥ria
    description: str             # Term√©k le√≠r√°sa
    technical_specs: Dict        # M≈±szaki specifik√°ci√≥k
    images: List[str]            # K√©p URL-ek
    documents: List[str]         # Dokumentum URL-ek
    price: Optional[float]       # √År (ha el√©rhet≈ë)
    availability: bool           # El√©rhet≈ës√©g
    scraped_at: datetime         # Scraping id≈ëpontja
```

### M≈±szaki specifik√°ci√≥k kinyer√©se

A scraper k√ºl√∂nb√∂z≈ë helyekr≈ël gy≈±jti a m≈±szaki adatokat:

1. **HTML t√°bl√°zatok** (`<table>`)
   - M≈±szaki adatok t√°bl√°zatok
   - Param√©ter-√©rt√©k p√°rok
   - Egys√©gek kezel√©se

2. **Defin√≠ci√≥s list√°k** (`<dl>`, `<dt>`, `<dd>`)
   - Struktur√°lt adatok
   - C√≠mke-√©rt√©k p√°rok

3. **Sz√∂veges feldolgoz√°s**
   - Regul√°ris kifejez√©sek
   - M√©rt√©kegys√©gek normaliz√°l√°sa

### K√©pek √©s dokumentumok

**K√©pek gy≈±jt√©se:**
- Term√©kfot√≥k azonos√≠t√°sa
- Gal√©ria k√©pek
- M≈±szaki rajzok
- Logo √©s dekorat√≠v k√©pek kisz≈±r√©se

**Dokumentumok:**
- PDF term√©kadatlapok
- M≈±szaki √∫tmutat√≥k
- Tan√∫s√≠tv√°nyok
- CAD f√°jlok

## üõ°Ô∏è Adatvalid√°l√°s

### Valid√°l√°si szab√°lyok

1. **K√∂telez≈ë mez≈ëk**
   - Term√©k n√©v (min 3 karakter)
   - URL (√©rv√©nyes form√°tum)
   - Kateg√≥ria (el≈ëre defini√°lt list√°b√≥l)

2. **Adatmin≈ës√©g ellen≈ërz√©s**
   - Le√≠r√°s hossza (min 10 karakter)
   - Gyan√∫s tartalmak kisz≈±r√©se
   - URL form√°tum valid√°l√°s

3. **Konzisztencia vizsg√°lat**
   - Kateg√≥ria-term√©k megfelel√©s
   - M≈±szaki adatok t√≠pusok
   - D√°tum valid√°l√°s

### Valid√°l√°si jelent√©s

```python
validator = DataValidator()
report = validator.get_validation_report(products)

print(f"√ârv√©nyes term√©kek: {report['summary']['valid']}")
print(f"Sikeres ar√°ny: {report['summary']['success_rate']:.1f}%")
print(f"Hi√°nyz√≥ le√≠r√°sok: {report['issues']['missing_descriptions']}")
```

## üîÑ Automatiz√°l√°s

### 1. Cron job be√°ll√≠t√°s

```bash
# Naponta egyszer, hajnali 2-kor
0 2 * * * /usr/bin/python3 /path/to/scraper_daily.py

# Hetente egyszer, vas√°rnap este
0 22 * * 0 /usr/bin/python3 /path/to/scraper_weekly.py
```

### 2. Celery feladatok (fejleszt√©s alatt)

```python
@celery_app.task
def scheduled_rockwool_scraping():
    scraper = RockwoolScraper()
    products = scraper.scrape_all_products()
    # Adatb√°zisba ment√©s...
    return len(products)
```

### 3. Docker k√∂rnyezetben

```yaml
# docker-compose.yml kieg√©sz√≠t√©s
scraper:
  build: ./backend
  command: python -m app.scraper.scheduled_runner
  environment:
    - SCRAPING_SCHEDULE=daily
  depends_on:
    - db
    - redis
```

## üß™ Tesztel√©s

### Automata tesztek futtat√°sa

```bash
# Backend k√∂nyvt√°rban
python -m app.scraper.test_scraper
```

### Tesztel√©si m√≥ddok

1. **Weboldal strukt√∫ra teszt**
   - Navig√°ci√≥ felt√©rk√©pez√©se
   - Kateg√≥ri√°k azonos√≠t√°sa
   - Term√©k linkek felder√≠t√©se

2. **Egyetlen term√©k teszt**
   - Minta term√©k scraping-je
   - Adatok valid√°l√°sa
   - Eredm√©ny ment√©se

3. **Kateg√≥ria teszt**
   - Korl√°tozott term√©ksz√°m
   - Valid√°l√°si statisztik√°k
   - Performance m√©r√©s

4. **Teljes scraping teszt**
   - Minden kateg√≥ria (korl√°tozott)
   - Bulk valid√°l√°s
   - Eredm√©nyek jelent√©se

## üìà Performance √©s optimaliz√°l√°s

### Rate Limiting strat√©gia

1. **Alapvet≈ë v√©delem**
   - 2-3 m√°sodperc k√©sleltet√©s k√©r√©sek k√∂z√∂tt
   - Maximum 20-30 k√©r√©s percenk√©nt
   - Exponenci√°lis backoff hib√°k eset√©n

2. **Rockwool specifikus**
   - Kateg√≥ri√°k k√∂z√∂tt 1 m√°sodperc v√°rakoz√°s
   - Term√©kek k√∂z√∂tt 0.5 m√°sodperc
   - IP alap√∫ blokkol√°s elker√ºl√©se

### Memory management

```python
# Nagy adatmennyis√©g eset√©n
products = []
for product_url in urls:
    product = scraper._scrape_single_product(product_url)
    if product:
        products.append(product)
    
    # Rendszeres tiszt√≠t√°s
    if len(products) % 100 == 0:
        # Adatb√°zisba ment√©s
        save_products_to_db(products)
        products.clear()
```

## üö® Hibakezel√©s

### Gyakori hib√°k √©s megold√°sok

1. **Connection timeout**
   ```python
   # Timeout n√∂vel√©se
   config.timeout = 60
   config.max_retries = 5
   ```

2. **Rate limiting (429 hiba)**
   ```python
   # Lassabb scraping
   config.request_delay = 5.0
   config.max_requests_per_minute = 10
   ```

3. **HTML strukt√∫ra v√°ltoz√°s**
   - ProductParser szelektorok friss√≠t√©se
   - √öj HTML elemek hozz√°ad√°sa
   - Tesztek √∫jrafuttat√°sa

### Logging √©s monitoring

```python
import logging

# R√©szletes logging be√°ll√≠t√°sa
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
```

## üîÆ J√∂v≈ëbeli fejleszt√©sek

### R√∂vid t√°v√∫ (1-2 h√©t)
- [ ] Adatb√°zis integr√°ci√≥ (ScrapedProduct -> Product model)
- [ ] Celery alap√∫ √ºtemezett feladatok
- [ ] R√©szletes hibakezel√©s √©s retry logika
- [ ] API authentik√°ci√≥ √©s rate limiting

### K√∂z√©pt√°v√∫ (1 h√≥nap)
- [ ] Incremental scraping (csak v√°ltoz√°sok)
- [ ] Multi-threading/async scraping
- [ ] Real-time monitoring dashboard
- [ ] Email riaszt√°sok hib√°k eset√©n

### Hossz√∫ t√°v√∫ (3+ h√≥nap)
- [ ] Tov√°bbi gy√°rt√≥k scraped (Isover, Knauf)
- [ ] Machine learning alap√∫ kategoriz√°l√°s
- [ ] Proxy rotation √©s IP v√°lt√°s
- [ ] Distributed scraping (t√∂bb szerver)

## üìû T√°mogat√°s

### Fejleszt≈ëi kapcsolat
- **Projekt:** Lambda.hu √âp√≠t≈ëanyag AI
- **F√°zis:** 2 - Adat-pipeline √©s Web Scraping
- **St√°tusz:** Rockwool scraper implement√°ci√≥ja k√©sz

### Hibabejelent√©s
1. Reproduk√°lhat√≥ hiba le√≠r√°sa
2. Scraper konfigur√°ci√≥
3. Log f√°jlok csatol√°sa
4. V√°rt vs. t√©nyleges eredm√©ny

### √öj funkci√≥ k√©r√©sek
- GitHub issues haszn√°lata
- R√©szletes use case le√≠r√°s
- Priority √©s timeline megjel√∂l√©se 