
import os
import json
import logging
from typing import Dict, List, Any

from anthropic import Anthropic

logger = logging.getLogger(__name__)


class ClaudeAIAnalyzer:
    """Real Claude AI analysis for PDF content - no simulations"""
    
    def __init__(self):
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError(
                "âŒ ANTHROPIC_API_KEY not found in environment variables"
            )
        
        self.client = Anthropic(api_key=api_key)
        self.model = "claude-3-5-haiku-20241022"
        
        logger.info(f"âœ… Claude AI initialized: {self.model}")
    
    async def claude_api_call(self, prompt: str) -> str:
        """Simple Claude API call for raw text extraction"""
        
        try:
            # Call Claude API directly with raw prompt
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                temperature=0.0,  # Low temperature for factual extraction
                system="I need losless extration from this PDF, without summarization, agrregation and compress of text",
                messages=[{"role": "user", "content": prompt}],
            )
            
            # Return raw response text
            response_text = response.content[0].text
            response_len = len(response_text)
            logger.info(f"âœ… Claude API call complete, length: {response_len}")
            return response_text
            
        except Exception as e:
            logger.error(f"âŒ Claude API call error: {e}")
            raise

    async def analyze_rockwool_pdf(
        self, text_content: str, tables_data: List[Dict], filename: str
    ) -> Dict[str, Any]:
        """Analyze ROCKWOOL PDF content with Claude AI"""
        
        # Prepare context for Claude
        context = self._prepare_analysis_context(text_content, tables_data, filename)
        
        # Create prompt for structured extraction
        prompt = self._create_extraction_prompt(context)
        
        try:
            # Call Claude API using the async method
            response_text = await self.claude_api_call(prompt)
            
            # Parse Claude's response
            analysis_result = self._parse_claude_response(response_text)
            
            logger.info(f"âœ… Claude analysis complete for {filename}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ Claude API error: {e}")
            # Return a safe fallback structure
            return {
                "product_identification": {},
                "technical_specifications": {},
                "extraction_metadata": {
                    "confidence_score": 0.0,
                    "extraction_method": "failed",
                    "error": str(e),
                    "found_fields": [],
                    "missing_fields": ["all"]
                }
            }
    
    def _prepare_analysis_context(
        self, text: str, tables: List[Dict], filename: str
    ) -> Dict[str, Any]:
        """Prepare context for Claude analysis"""
        
        context = {
            "filename": filename,
            "text_length": len(text),
            "tables_count": len(tables),
            "extracted_text": text[:8000],  # Limit text for API
            "tables_summary": [],
        }
        
        # Summarize tables
        for table in tables[:5]:  # Limit to first 5 tables
            table_summary = {
                "page": table.get("page", "unknown"),
                "headers": table.get("headers", []),
                "row_count": len(table.get("data", [])),
                "sample_data": table.get("data", [])[:3],  # First 3 rows
            }
            context["tables_summary"].append(table_summary)
        
        return context
    
    def _create_extraction_prompt(self, context: Dict[str, Any]) -> str:
        """Create a dynamic, adaptive prompt for Claude based on actual PDF content."""
        
        # Analyze the PDF content structure first
        content_analysis = self._analyze_pdf_structure(context)
        
        # Build adaptive prompt based on what we actually found
        prompt = f"""
ðŸ” DINAMIKUS TARTALOM ELEMZÃ‰S - '{context['filename']}'

ðŸ“Š AUTOMATIKUS STRUKTÃšRA FELISMERÃ‰S:
{content_analysis['structure_summary']}

ðŸŽ¯ ADAPTÃV KINYERÃ‰SI STRATÃ‰GIA:
{content_analysis['extraction_strategy']}

ðŸ“‹ TALÃLT TARTALOM TÃPUSOK:
{content_analysis['content_type']}

DOKUMENTUM TARTALOM:
{context['extracted_text']}
"""
        
        # Add tables only if they exist and are meaningful
        if context["tables_summary"] and content_analysis['has_meaningful_tables']:
            prompt += f"\n\nðŸ” TÃBLÃZAT ADATOK ({len(context['tables_summary'])} db):\n"
            for i, table in enumerate(context["tables_summary"], 1):
                prompt += f"\nTÃ¡blÃ¡zat {i}:\n"
                prompt += f"FejlÃ©cek: {table['headers']}\n"
                prompt += f"Adatok: {table['sample_data']}\n"
        
        # Dynamic field mapping based on content
        field_mapping = self._generate_dynamic_field_mapping(content_analysis)
        prompt += f"\n\nðŸ—ºï¸ DINAMIKUS MEZÅ LEKÃ‰PEZÃ‰S:\n{field_mapping}\n"
        
        # Adaptive extraction instructions
        prompt += f"""
ðŸ¤– ADAPTÃV KINYERÃ‰SI UTASÃTÃSOK:

1. TARTALOM ALAPÃš MEGKÃ–ZELÃTÃ‰S:
   - Elemezd a tÃ©nyleges tartalmat, ne keress elÅ‘re definiÃ¡lt mezÅ‘ket
   - AzonosÃ­tsd a mÅ±szaki adatokat bÃ¡rhol is legyenek
   - HasznÃ¡ld a kontextust az adatok Ã©rtelmezÃ©sÃ©hez

2. DINAMIKUS STRUKTÃšRA KEZELÃ‰S:
   - Ha tÃ¡blÃ¡zat van â†’ prioritÃ¡s a tÃ¡blÃ¡zat adatoknak
   - Ha csak szÃ¶veg van â†’ regex Ã©s pattern matching
   - Ha vegyes tartalom â†’ kombinÃ¡ld a mÃ³dszereket

3. ADAPTÃV KIMENETI SÃ‰MA:
   - Csak azokat a mezÅ‘ket add vissza, amik tÃ©nylegesen megtalÃ¡lhatÃ³k
   - Null Ã©rtÃ©kek helyett hagyd ki a hiÃ¡nyzÃ³ mezÅ‘ket
   - Confidence score legyen reÃ¡lis a talÃ¡lt adatok alapjÃ¡n

4. MINÅSÃ‰GI VALIDÃCIÃ“:
   - EllenÅ‘rizd az Ã©rtÃ©kek konzisztenciÃ¡jÃ¡t
   - MÃ©rtÃ©kegysÃ©gek Ã©s szabvÃ¡nyok pontossÃ¡ga
   - Logikus Ã©rtÃ©ktartomÃ¡nyok (pl. Î»: 0.02-0.1 W/mK)

ðŸ“¤ KIMENETI JSON FORMÃTUM (DINAMIKUS):
{{
  "product_identification": {{
    // Csak a tÃ©nylegesen talÃ¡lt adatok
    {content_analysis['likely_fields']['product_identification']}
  }},
  "technical_specifications": {{
    // Csak a PDF-ben szereplÅ‘ mÅ±szaki adatok
    {content_analysis['likely_fields']['technical_specifications']}
  }},
  "extraction_metadata": {{
    "content_type": "{content_analysis['content_type']}",
    "extraction_method": "{content_analysis['best_extraction_method']}",
    "data_completeness": "0.0-1.0 kÃ¶zÃ¶tt",
    "confidence_score": "0.0-1.0 kÃ¶zÃ¶tt",
    "found_fields": ["lista", "a", "talÃ¡lt", "mezÅ‘krÅ‘l"],
    "missing_fields": ["lista", "a", "hiÃ¡nyzÃ³", "mezÅ‘krÅ‘l"]
  }}
}}

âš¡ KRITIKUS: Ne hasznÃ¡lj sablont! Elemezd a tÃ©nyleges tartalmat Ã©s csak azt 
add vissza, amit tÃ©nylegesen megtalÃ¡lsz!
"""
        
        return prompt
    
    def _analyze_pdf_structure(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze the actual PDF structure to determine extraction strategy."""
        
        text = context['extracted_text'].lower()
        tables = context['tables_summary']
        
        analysis = {
            'content_type': 'unknown',
            'has_meaningful_tables': False,
            'extraction_strategy': 'text_based',
            'structure_summary': '',
            'likely_fields': {
                'product_identification': {},
                'technical_specifications': {}
            },
            'best_extraction_method': 'pattern_matching'
        }
        
        # Detect content type
        if 'rockwool' in text or 'hÅ‘szigetelÃ©s' in text:
            analysis['content_type'] = 'insulation_datasheet'
        elif 'Ã¡rlista' in text or 'Ã¡r' in text:
            analysis['content_type'] = 'price_list'
        elif 'mÅ±szaki' in text or 'technical' in text:
            analysis['content_type'] = 'technical_spec'
        
        # Analyze tables
        if tables:
            meaningful_tables = 0
            for table in tables:
                if len(table.get('headers', [])) > 1 and len(table.get('sample_data', [])) > 0:
                    meaningful_tables += 1
            
            if meaningful_tables > 0:
                analysis['has_meaningful_tables'] = True
                analysis['extraction_strategy'] = 'table_based'
                analysis['best_extraction_method'] = 'table_parsing'
        
        # Detect likely fields based on content
        tech_specs = analysis['likely_fields']['technical_specifications']
        if 'Î»' in text or 'hÅ‘vezetÃ©si' in text:
            tech_specs['thermal_conductivity'] = 'expected'
        if 'tÅ±zvÃ©delmi' in text or 'fire' in text:
            tech_specs['fire_classification'] = 'expected'
        if 'testsÅ±rÅ±sÃ©g' in text or 'density' in text:
            tech_specs['density'] = 'expected'
        
        # Generate structure summary
        analysis['structure_summary'] = f"""
- Tartalom tÃ­pus: {analysis['content_type']}
- TÃ¡blÃ¡zatok: {len(tables)} db ({'jelentÅ‘s' if analysis['has_meaningful_tables'] else 'kevÃ©s adat'})
- SzÃ¶veg hossz: {context['text_length']} karakter
- AjÃ¡nlott mÃ³dszer: {analysis['best_extraction_method']}
"""
        
        return analysis
    
    def _generate_dynamic_field_mapping(self, analysis: Dict[str, Any]) -> str:
        """Generate field mapping based on content analysis."""
        
        mapping = "DINAMIKUS MEZÅ FELISMERÃ‰S:\n"
        
        # Hungarian technical terms that might appear
        hungarian_terms = {
            'hÅ‘vezetÃ©si tÃ©nyezÅ‘': 'thermal_conductivity',
            'Î»d': 'thermal_conductivity', 
            'lambda': 'thermal_conductivity',
            'tÅ±zvÃ©delmi osztÃ¡ly': 'fire_classification',
            'testsÅ±rÅ±sÃ©g': 'density',
            'Ï': 'density',
            'nyomÃ³szilÃ¡rdsÃ¡g': 'compressive_strength',
            'pÃ¡radiffÃºziÃ³s': 'vapor_resistance',
            'Î¼': 'vapor_resistance',
            'vÃ­zfelvÃ©tel': 'water_absorption',
            'ws': 'water_absorption',
            'wl': 'water_absorption'
        }
        
        for hu_term, en_key in hungarian_terms.items():
            mapping += f"- '{hu_term}' â†’ {en_key}\n"
        
        mapping += "\nðŸŽ¯ ADAPTÃV STRATÃ‰GIA: Keress hasonlÃ³ kifejezÃ©seket Ã©s Ã©rtelmezd a kontextus alapjÃ¡n!"
        
        return mapping
    
    def _parse_claude_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Claude's dynamic JSON response - adapts to actual content"""
        
        logger.debug(f"ðŸ“ Parsing Claude response (length: {len(response_text)})")
        
        # âœ… ENHANCED: More flexible JSON pattern matching for Claude responses
        json_patterns = [
            (r'\{(?:[^{}]*\{[^{}]*\}[^{}]*)*[^{}]*\}', "complete JSON object"),
            (r'```json\s*(\{.*?\})\s*```', "JSON code block"),
            (r'```\s*(\{.*?\})\s*```', "code block"),
            (r'\{[\s\S]*?\}(?=\s*$|\s*\n\s*[A-Z])', "JSON at end of response"),
            (r'(?:json|JSON)[\s:]*(\{[\s\S]*?\})', "labeled JSON block"),
        ]
        
        for pattern, description in json_patterns:
            import re
            matches = re.findall(pattern, response_text, re.DOTALL)
            for match in matches:
                try:
                    # Clean the JSON text
                    json_text = match.strip()
                    if json_text.startswith('json'):
                        json_text = json_text[4:].strip()
                    
                    result = json.loads(json_text)
                    logger.info(f"âœ… JSON parsed successfully using {description}")
                    
                    # Dynamic validation - only check for what we actually expect
                    self._validate_dynamic_structure(result)
                    
                    # Enhance with extraction metadata
                    result = self._enhance_with_metadata(result)
                    
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.debug(f"âŒ Failed to parse {description}: {e}")
                    continue
        
        # Fallback: look for simple JSON block boundaries
        try:
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                
                # Try to fix common JSON issues
                json_text = self._clean_json_text(json_text)
                
                result = json.loads(json_text)
                logger.info("âœ… JSON parsed using fallback method")
                
                # Dynamic validation and enhancement
                self._validate_dynamic_structure(result)
                result = self._enhance_with_metadata(result)
                
                return result
            else:
                logger.warning("âš ï¸ No JSON boundaries found in response")
                
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON parsing failed: {e}")
            logger.debug(f"Problematic JSON text: {json_text[:200]}...")
        except Exception as e:
            logger.error(f"âŒ Unexpected error during JSON parsing: {e}")
        
        # Final fallback: return error structure
        logger.warning("âš ï¸ Using fallback response structure")
        return {
            "product_identification": {},
            "technical_specifications": {},
            "extraction_metadata": {
                "confidence_score": 0.0,
                "extraction_method": "failed",
                "error": "No valid JSON found in Claude response",
                "found_fields": [],
                "missing_fields": ["all"],
                "response_preview": response_text[:200] + "..." if len(response_text) > 200 else response_text
            }
        }
    
    def _clean_json_text(self, json_text: str) -> str:
        """âœ… ENHANCED: Clean JSON text to fix common Claude AI response issues"""
        
        import re
        
        # Remove any trailing commas before closing braces/brackets
        json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
        
        # Replace single quotes with double quotes (common AI mistake)
        json_text = re.sub(r"'([^']*)':", r'"\1":', json_text)
        json_text = re.sub(r":\s*'([^']*)'", r': "\1"', json_text)
        
        # Fix Hungarian characters and Unicode issues
        hungarian_fixes = {
            '\u00e9': 'Ã©', '\u0151': 'Å‘', '\u00f3': 'Ã³', '\u00e1': 'Ã¡',
            '\u00f6': 'Ã¶', '\u00fc': 'Ã¼', '\u00ed': 'Ã­', '\u00fa': 'Ãº', 
            '\u0171': 'Å±', '\u0170': 'Å°', '\u0150': 'Å'
        }
        for unicode_char, normal_char in hungarian_fixes.items():
            json_text = json_text.replace(unicode_char, normal_char)
        
        # Remove comments and explanatory text
        json_text = re.sub(r'//.*$', '', json_text, flags=re.MULTILINE)
        json_text = re.sub(r'/\*.*?\*/', '', json_text, flags=re.DOTALL)
        
        # Remove common Claude explanation prefixes
        json_text = re.sub(r'^[^{]*(?=\{)', '', json_text)
        json_text = re.sub(r'\}[^}]*$', '}', json_text)
        
        # Fix newlines and spacing
        json_text = re.sub(r'\n\s*\n', '\n', json_text)
        
        return json_text.strip()
    
    def _validate_dynamic_structure(self, result: Dict[str, Any]) -> None:
        """Validate structure dynamically based on what was actually found"""
        
        # Ensure basic structure exists
        if "product_identification" not in result:
            result["product_identification"] = {}
        if "technical_specifications" not in result:
            result["technical_specifications"] = {}
        if "extraction_metadata" not in result:
            result["extraction_metadata"] = {}
        
        # Validate technical specifications format
        tech_specs = result.get("technical_specifications", {})
        for key, value in tech_specs.items():
            if isinstance(value, dict):
                # Ensure proper structure for technical values
                if "value" not in value:
                    logger.warning(f"Technical spec {key} missing 'value' field")
                if "unit" not in value:
                    logger.warning(f"Technical spec {key} missing 'unit' field")
    
    def _enhance_with_metadata(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Enhance result with extraction metadata"""
        
        from datetime import datetime

        metadata = result.get("extraction_metadata", {})
        
        # Count found fields
        found_fields = []
        if result.get("product_identification"):
            found_fields.extend(result["product_identification"].keys())
        if result.get("technical_specifications"):
            found_fields.extend(result["technical_specifications"].keys())
        
        # Calculate data completeness
        expected_fields = [
            "product_identification.name",
            "technical_specifications.thermal_conductivity",
            "technical_specifications.fire_classification",
            "technical_specifications.density"
        ]
        
        completeness = len(found_fields) / len(expected_fields) if expected_fields else 0
        
        # Update metadata
        metadata.update({
            "found_fields": found_fields,
            "data_completeness": round(completeness, 2),
            "extraction_timestamp": datetime.now().isoformat()
        })
        
        result["extraction_metadata"] = metadata
        
        return result 